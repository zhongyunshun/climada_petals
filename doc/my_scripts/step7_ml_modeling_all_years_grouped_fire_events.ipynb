{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-25T02:13:33.965600Z",
     "start_time": "2024-07-25T02:13:33.748601Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Load relevant functions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9012436159c5902f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1.1 Load the function to group the DataFrame into fire events"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "29b3b14f360028ce"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def group_into_fire_events(df):\n",
    "    \"\"\"\n",
    "    This function takes in a DataFrame and returns a copy of the DataFrame with three additional columns:\n",
    "    'date_cluster', 'regional_cluster', and 'event_id'.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        df (pd.DataFrame): The output DataFrame with three additional columns: 'date_cluster', 'regional_cluster', and 'event_id'.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.copy()\n",
    "    df.rename(columns={'latitude_left': 'latitude', 'longitude_left': 'longitude'}, inplace=True)\n",
    "    \n",
    "    # Convert 'acq_date' to datetime\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    df = df.sort_values(by='date')\n",
    "    \n",
    "    # Create 'date_cluster' column based on consecutive dates\n",
    "    df['date_cluster'] = (df['date'].diff().dt.days > 1).cumsum()\n",
    "    \n",
    "    # Function to apply DBSCAN on each consecutive date cluster\n",
    "    def apply_dbscan(group):\n",
    "        coords = group[['latitude', 'longitude']].values \n",
    "        # Apply DBSCAN, hyperparameters are the same as Climada\n",
    "        db = DBSCAN(eps=15/111.12, min_samples=1).fit(coords)\n",
    "        group['regional_cluster'] = db.labels_\n",
    "        return group\n",
    "    \n",
    "    # Apply DBSCAN on each date cluster\n",
    "    df = df.groupby('date_cluster').apply(apply_dbscan)\n",
    "    \n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Create a unique 'event_id' for each unique combination of 'date_cluster' and 'regional_cluster'\n",
    "    df['event_id'] = df.groupby(['date_cluster', 'regional_cluster']).ngroup()\n",
    "    \n",
    "    df = df.sort_values(by='event_id').reset_index(drop=True)\n",
    "    \n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-25T02:13:33.980600Z",
     "start_time": "2024-07-25T02:13:33.966601Z"
    }
   },
   "id": "e671d22827bb58f1",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1.2 Load the function to shuffle and split data into training and testing according to event_id"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "24844bc42599e48a"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def split_event_ids(event_id_pairs, test_size, random_seed):\n",
    "    \"\"\"\n",
    "    Splits event_id pairs into training and test sets based on the specified test size.\n",
    "\n",
    "    Args:\n",
    "        event_id_pairs (list of tuples): List of event_id and their corresponding row counts.\n",
    "        test_size (float): The proportion of the test set.\n",
    "        random_seed (int): The seed for the random number generator.\n",
    "\n",
    "    Returns:\n",
    "        train_event_ids (list): List of event_ids for the training set.\n",
    "        test_event_ids (list): List of event_ids for the test set.\n",
    "    \"\"\"\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(event_id_pairs)\n",
    "    \n",
    "    test_event_ids = []\n",
    "    current_test_rows = 0\n",
    "    total_rows = sum(count for _, count in event_id_pairs)\n",
    "    test_rows_target = int(total_rows * test_size)\n",
    "    \n",
    "    for event_id, count in event_id_pairs:\n",
    "        current_test_rows += count\n",
    "        test_event_ids.append(event_id)\n",
    "        if current_test_rows >= test_rows_target:\n",
    "            break\n",
    "    \n",
    "    train_event_ids = [event_id for event_id, _ in event_id_pairs if event_id not in test_event_ids]\n",
    "    \n",
    "    return train_event_ids, test_event_ids\n",
    "\n",
    "def shuffle_and_split(df, test_size=0.1, random_seed=42):\n",
    "    \"\"\"\n",
    "    This function shuffles the DataFrame with respect to 'event_id' and then splits the data\n",
    "    into training and test sets with approximately 10% test size with respect to 'event_id'.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame\n",
    "        test_size (float): The proportion of the test set\n",
    "        random_seed (int): The seed for the random number generator\n",
    "        \n",
    "    Returns:\n",
    "        X_train (pd.DataFrame): Training features\n",
    "        X_test (pd.DataFrame): Test features\n",
    "        y_train (pd.Series): Training labels\n",
    "        y_test (pd.Series): Test labels\n",
    "    \"\"\"\n",
    "    # Shuffle the dataframe based on event_id\n",
    "    event_counts = df['event_id'].value_counts().sort_index()\n",
    "    event_id_pairs = list(zip(event_counts.index, event_counts.values))\n",
    "    \n",
    "    train_event_ids, test_event_ids = split_event_ids(event_id_pairs, test_size, random_seed)\n",
    "    \n",
    "    X_train = df[df['event_id'].isin(train_event_ids)].drop(columns=['ignited'])\n",
    "    y_train = df[df['event_id'].isin(train_event_ids)]['ignited']\n",
    "    X_test = df[df['event_id'].isin(test_event_ids)].drop(columns=['ignited'])\n",
    "    y_test = df[df['event_id'].isin(test_event_ids)]['ignited']\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# folder = '../../climada_petals/data/wildfire/outputs/'\n",
    "# year = 2013\n",
    "# file_path = os.path.join(folder, str(year), f'ignited_eu_{year}_gdf')\n",
    "#     \n",
    "# # Load the DataFrame from the file\n",
    "# df = gpd.read_file(file_path)\n",
    "# X_train, X_test, y_train, y_test = shuffle_and_split(df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-25T02:19:24.575958Z",
     "start_time": "2024-07-25T02:17:19.409847Z"
    }
   },
   "id": "54ca58bd7ec20886",
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c9c8a89097f6579"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def custom_cv_split(X, y, n_splits=5, split_by_col='event_id', random_state=42):\n",
    "    \"\"\"\n",
    "    This function creates cross-validation splits based on a specified column (e.g., 'event_id'),\n",
    "    ensuring that all rows with the same value in the specified column are kept together in the same fold.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): Features\n",
    "        y (pd.Series): Labels\n",
    "        n_splits (int): Number of folds\n",
    "        split_by_col (str): Column name to split by (e.g., 'event_id')\n",
    "        random_state (int): Random seed for reproducibility\n",
    "\n",
    "    Yields:\n",
    "        train_indices, val_indices (np.ndarray): Indices for training and validation sets for each fold\n",
    "    \"\"\"\n",
    "    # Ensure reproducibility\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Get unique event IDs and shuffle them\n",
    "    unique_event_ids = X[split_by_col].unique()\n",
    "    np.random.shuffle(unique_event_ids)\n",
    "    \n",
    "    # Calculate the fold sizes\n",
    "    fold_sizes = np.full(n_splits, len(unique_event_ids) // n_splits, dtype=int)\n",
    "    fold_sizes[:len(unique_event_ids) % n_splits] += 1\n",
    "    \n",
    "    current = 0\n",
    "    for fold_size in fold_sizes:\n",
    "        val_event_ids = unique_event_ids[current:current + fold_size]\n",
    "        train_event_ids = np.setdiff1d(unique_event_ids, val_event_ids)\n",
    "        \n",
    "        train_indices = X[X[split_by_col].isin(train_event_ids)].index\n",
    "        val_indices = X[X[split_by_col].isin(val_event_ids)].index\n",
    "        \n",
    "        yield train_indices, val_indices\n",
    "        current += fold_size\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-25T02:13:35.809539Z",
     "start_time": "2024-07-25T02:13:35.805193Z"
    }
   },
   "id": "2d2bf4826e354f03",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# \n",
    "# def custom_cv_split(X, y, n_splits=5, random_state=42):\n",
    "#     unique_event_ids = X['event_id'].unique()\n",
    "#     np.random.seed(random_state)\n",
    "#     np.random.shuffle(unique_event_ids)\n",
    "#     fold_sizes = np.full(n_splits, len(unique_event_ids) // n_splits, dtype=int)\n",
    "#     fold_sizes[:len(unique_event_ids) % n_splits] += 1\n",
    "#     current = 0\n",
    "#     for fold_size in fold_sizes:\n",
    "#         test_event_ids = unique_event_ids[current:current + fold_size]\n",
    "#         train_event_ids = np.setdiff1d(unique_event_ids, test_event_ids)\n",
    "#         train_indices = X[X['event_id'].isin(train_event_ids)].index\n",
    "#         test_indices = X[X['event_id'].isin(test_event_ids)].index\n",
    "#         yield train_indices, test_indices\n",
    "#         current += fold_size\n",
    "# \n",
    "# def test_custom_cv_split():\n",
    "#     # Generate a sample dataframe\n",
    "#     data = {\n",
    "#         'event_id': [0, 0, 0, 1, 2, 2, 3, 3, 4, 5, 5, 5, 6, 6, 7, 7, 7, 7, 8, 8, 8, 8],\n",
    "#         'feature1': range(22),\n",
    "#         'feature2': range(22, 44),\n",
    "#         'ignited': [1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1]\n",
    "#     }\n",
    "#     df = pd.DataFrame(data)\n",
    "#     X = df.drop(columns=['ignited'])\n",
    "#     y = df['ignited']\n",
    "#     \n",
    "#     # Test 1: Each split respects the event_id boundaries\n",
    "#     for train_indices, test_indices in custom_cv_split(X, y, n_splits=5):\n",
    "#         train_event_ids = X.loc[train_indices, 'event_id'].unique()\n",
    "#         test_event_ids = X.loc[test_indices, 'event_id'].unique()\n",
    "#         assert len(set(train_event_ids).intersection(set(test_event_ids))) == 0, \"Overlap of event_ids between train and test sets\"\n",
    "#     \n",
    "#     # Test 2: There is no overlap between training and validation sets for each split\n",
    "#     for train_indices, test_indices in custom_cv_split(X, y, n_splits=5):\n",
    "#         assert len(set(train_indices).intersection(set(test_indices))) == 0, \"Overlap of indices between train and test sets\"\n",
    "#     \n",
    "#     # Test 3: The number of splits is correct\n",
    "#     splits = list(custom_cv_split(X, y, n_splits=5))\n",
    "#     assert len(splits) == 5, \"Number of splits is incorrect\"\n",
    "#     \n",
    "#     # Test 4: The total number of rows in the splits matches the total number of rows in the input data\n",
    "#     test_indices_combined = np.concatenate([test_indices for _, test_indices in splits])\n",
    "#     assert len(test_indices_combined) == len(X), \"Total number of rows in the splits does not match the total number of rows in the input data\"\n",
    "#     \n",
    "#     # Test 5: Handle small number of splits\n",
    "#     small_splits = list(custom_cv_split(X, y, n_splits=2))\n",
    "#     assert len(small_splits) == 2, \"Small number of splits test failed: Number of splits is incorrect\"\n",
    "#     \n",
    "#     # Test 6: Correct splitting for small dataset\n",
    "#     small_data = {\n",
    "#         'event_id': [0, 0, 1, 1],\n",
    "#         'feature1': [10, 20, 30, 40],\n",
    "#         'feature2': [50, 60, 70, 80],\n",
    "#         'ignited': [1, 0, 1, 0]\n",
    "#     }\n",
    "#     small_df = pd.DataFrame(small_data)\n",
    "#     X_small = small_df.drop(columns=['ignited'])\n",
    "#     y_small = small_df['ignited']\n",
    "#     small_splits = list(custom_cv_split(X_small, y_small, n_splits=2))\n",
    "#     for train_indices, test_indices in small_splits:\n",
    "#         train_event_ids = X_small.loc[train_indices, 'event_id'].unique()\n",
    "#         test_event_ids = X_small.loc[test_indices, 'event_id'].unique()\n",
    "#         assert len(set(train_event_ids).intersection(set(test_event_ids))) == 0, \"Small dataset test failed: Overlap of event_ids between train and test sets\"\n",
    "#     \n",
    "#     print(\"All tests passed!\")\n",
    "# \n",
    "# # Run the tests\n",
    "# test_custom_cv_split()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-23T03:26:33.780376Z",
     "start_time": "2024-07-23T03:26:33.759376Z"
    }
   },
   "id": "972bc9bdab062320",
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1.3 Load the function to train and evaluate ML models"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e155bdf92dde1424"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_and_evaluate_models(df, random_state=42):\n",
    "    # Preprocess the dataframe\n",
    "    df = pd.get_dummies(df, columns=['land_cover'], prefix='land_cover')\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    df = df.drop(columns=['month', 'date', 'distance_km', 'confidence', 'geometry'])\n",
    "    df['fwi'].fillna(0, inplace=True)\n",
    "\n",
    "    # Split the data\n",
    "    X_train_val, X_test, y_train_val, y_test = shuffle_and_split(df, test_size=0.1, random_seed=random_state)\n",
    "    X_train_val = X_train_val.drop(columns=['latitude', 'longitude', 'brightness', 'bright_t31'])\n",
    "    X_test = X_test.drop(columns=['latitude', 'longitude', 'brightness', 'bright_t31'])\n",
    "\n",
    "    # Initialize dictionaries to store the best models, hyperparameters, and scores\n",
    "    classifier_names = [\"LogisticRegression\", \"XGBClassifier\"]\n",
    "    best_models = {name: None for name in classifier_names}\n",
    "    best_params = {name: None for name in classifier_names}\n",
    "    best_scores = {name: 0 for name in classifier_names}\n",
    "\n",
    "    # Define objective functions for Optuna\n",
    "    def logisticregression_objective(trial):\n",
    "        classifier_name = \"LogisticRegression\"\n",
    "        logistic_c = trial.suggest_float('logistic_c', 1e-5, 1e5, log=True)\n",
    "        classifier_obj = LogisticRegression(C=logistic_c, random_state=random_state)\n",
    "        model_pipeline = make_pipeline(StandardScaler(), classifier_obj)\n",
    "        scores = cross_val_score(model_pipeline, X_train_val, y_train_val, cv=list(custom_cv_split(X_train_val, y_train_val, n_splits=5, split_by_col='event_id', random_state=random_state)), n_jobs=-1, scoring='roc_auc')\n",
    "        score = scores.mean()\n",
    "        if score > best_scores[classifier_name]:\n",
    "            best_scores[classifier_name] = score\n",
    "            best_params[classifier_name] = trial.params\n",
    "            best_models[classifier_name] = classifier_obj # classifier_obj remain untrained\n",
    "        return score\n",
    "\n",
    "    def xgboost_objective(trial):\n",
    "        classifier_name = \"XGBClassifier\"\n",
    "        xgb_params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 200, 2000),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 1e-3, 1e-1, log=True),\n",
    "            'subsample': trial.suggest_float('subsample', 0.2, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "            'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 0, 1),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 0, 1),\n",
    "            'random_state': random_state\n",
    "        }\n",
    "        classifier_obj = XGBClassifier(**xgb_params)\n",
    "        scores = cross_val_score(classifier_obj, X_train_val, y_train_val, cv=list(custom_cv_split(X_train_val, y_train_val, n_splits=5, split_by_col='event_id', random_state=random_state)), n_jobs=-1, scoring='roc_auc')\n",
    "        score = scores.mean()\n",
    "        if score > best_scores[classifier_name]:\n",
    "            best_scores[classifier_name] = score\n",
    "            best_params[classifier_name] = trial.params\n",
    "            best_models[classifier_name] = classifier_obj # classifier_obj remain untrained\n",
    "        return score\n",
    "\n",
    "    # Optimize hyperparameters with Optuna\n",
    "    study = optuna.create_study(sampler=optuna.samplers.TPESampler(), direction='maximize')\n",
    "    study.optimize(logisticregression_objective, n_trials=20)\n",
    "    study.optimize(xgboost_objective, n_trials=100)\n",
    "\n",
    "    results = {}\n",
    "    for name in classifier_names:\n",
    "        if name == \"LogisticRegression\":\n",
    "            best_models[name].set_params(max_iter=200)\n",
    "            model_pipeline = make_pipeline(StandardScaler(), best_models[name])\n",
    "        else:\n",
    "            model_pipeline = best_models[name]\n",
    "\n",
    "        model_pipeline.fit(X_train_val, y_train_val)\n",
    "        train_accuracy = model_pipeline.score(X_train_val, y_train_val)\n",
    "        test_accuracy = model_pipeline.score(X_test, y_test)\n",
    "        y_pred = model_pipeline.predict(X_test)\n",
    "        classification_rep = classification_report(y_test, y_pred)\n",
    "        confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "        results[name] = {\n",
    "            \"train_accuracy\": train_accuracy,\n",
    "            \"test_accuracy\": test_accuracy,\n",
    "            \"classification_report\": classification_rep,\n",
    "            \"confusion_matrix\": confusion_mat\n",
    "        }\n",
    "\n",
    "        if name == \"LogisticRegression\":\n",
    "            feature_importance = best_models[name].coef_[0]\n",
    "        else:\n",
    "            feature_importance = best_models[name].feature_importances_\n",
    "\n",
    "        feature_importance_df = pd.DataFrame({'feature': X_train_val.columns, 'importance': feature_importance})\n",
    "        feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)\n",
    "        results[name][\"feature_importance\"] = feature_importance_df\n",
    "\n",
    "    return results\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-25T02:13:38.162225Z",
     "start_time": "2024-07-25T02:13:38.142225Z"
    }
   },
   "id": "c7cf786755d721",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Run ML model for each year"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fd5579c337435486"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "folder = '../../climada_petals/data/wildfire/outputs/'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-25T02:14:40.539791Z",
     "start_time": "2024-07-25T02:14:40.534791Z"
    }
   },
   "id": "78ebdf6130b89ba4",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists(folder)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-25T02:14:40.967844Z",
     "start_time": "2024-07-25T02:14:40.950845Z"
    }
   },
   "id": "3f63eede1e53e28b",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-24 22:14:55,359] A new study created in memory with name: no-name-f9b40153-cda1-4872-bb56-5a4fea779556\n",
      "[W 2024-07-24 22:14:56,539] Trial 0 failed with parameters: {'logistic_c': 27.965002487341778} because of the following error: IndexError('indices are out-of-bounds').\n",
      "joblib.externals.loky.process_executor._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\zhong\\.conda\\envs\\climada_env\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 463, in _process_worker\n",
      "    r = call_item()\n",
      "  File \"C:\\Users\\zhong\\.conda\\envs\\climada_env\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 291, in __call__\n",
      "    return self.fn(*self.args, **self.kwargs)\n",
      "  File \"C:\\Users\\zhong\\.conda\\envs\\climada_env\\lib\\site-packages\\joblib\\parallel.py\", line 598, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\zhong\\.conda\\envs\\climada_env\\lib\\site-packages\\joblib\\parallel.py\", line 598, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\zhong\\.conda\\envs\\climada_env\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 129, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\zhong\\.conda\\envs\\climada_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 887, in _fit_and_score\n",
      "    X_train, y_train = _safe_split(estimator, X, y, train)\n",
      "  File \"C:\\Users\\zhong\\.conda\\envs\\climada_env\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\", line 158, in _safe_split\n",
      "    X_subset = _safe_indexing(X, indices)\n",
      "  File \"C:\\Users\\zhong\\.conda\\envs\\climada_env\\lib\\site-packages\\sklearn\\utils\\__init__.py\", line 407, in _safe_indexing\n",
      "    return _pandas_indexing(X, indices, indices_dtype, axis=axis)\n",
      "  File \"C:\\Users\\zhong\\.conda\\envs\\climada_env\\lib\\site-packages\\sklearn\\utils\\__init__.py\", line 219, in _pandas_indexing\n",
      "    return X.take(key, axis=axis)\n",
      "  File \"C:\\Users\\zhong\\.conda\\envs\\climada_env\\lib\\site-packages\\pandas\\core\\generic.py\", line 4068, in take\n",
      "    new_data = self._mgr.take(\n",
      "  File \"C:\\Users\\zhong\\.conda\\envs\\climada_env\\lib\\site-packages\\pandas\\core\\internals\\managers.py\", line 874, in take\n",
      "    indexer = maybe_convert_indices(indexer, n, verify=verify)\n",
      "  File \"C:\\Users\\zhong\\.conda\\envs\\climada_env\\lib\\site-packages\\pandas\\core\\indexers\\utils.py\", line 282, in maybe_convert_indices\n",
      "    raise IndexError(\"indices are out-of-bounds\")\n",
      "IndexError: indices are out-of-bounds\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\zhong\\.conda\\envs\\climada_env\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\zhong\\AppData\\Local\\Temp\\ipykernel_13640\\2043175876.py\", line 28, in logisticregression_objective\n",
      "    scores = cross_val_score(model_pipeline, X_train_val, y_train_val, cv=list(custom_cv_split(X_train_val, y_train_val, n_splits=5, split_by_col='event_id', random_state=random_state)), n_jobs=-1, scoring='roc_auc')\n",
      "  File \"C:\\Users\\zhong\\.conda\\envs\\climada_env\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 213, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\zhong\\.conda\\envs\\climada_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 719, in cross_val_score\n",
      "    cv_results = cross_validate(\n",
      "  File \"C:\\Users\\zhong\\.conda\\envs\\climada_env\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 213, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\zhong\\.conda\\envs\\climada_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 430, in cross_validate\n",
      "    results = parallel(\n",
      "  File \"C:\\Users\\zhong\\.conda\\envs\\climada_env\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 67, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"C:\\Users\\zhong\\.conda\\envs\\climada_env\\lib\\site-packages\\joblib\\parallel.py\", line 2007, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "  File \"C:\\Users\\zhong\\.conda\\envs\\climada_env\\lib\\site-packages\\joblib\\parallel.py\", line 1650, in _get_outputs\n",
      "    yield from self._retrieve()\n",
      "  File \"C:\\Users\\zhong\\.conda\\envs\\climada_env\\lib\\site-packages\\joblib\\parallel.py\", line 1754, in _retrieve\n",
      "    self._raise_error_fast()\n",
      "  File \"C:\\Users\\zhong\\.conda\\envs\\climada_env\\lib\\site-packages\\joblib\\parallel.py\", line 1789, in _raise_error_fast\n",
      "    error_job.get_result(self.timeout)\n",
      "  File \"C:\\Users\\zhong\\.conda\\envs\\climada_env\\lib\\site-packages\\joblib\\parallel.py\", line 745, in get_result\n",
      "    return self._return_or_raise()\n",
      "  File \"C:\\Users\\zhong\\.conda\\envs\\climada_env\\lib\\site-packages\\joblib\\parallel.py\", line 763, in _return_or_raise\n",
      "    raise self._result\n",
      "IndexError: indices are out-of-bounds\n",
      "[W 2024-07-24 22:14:56,552] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "indices are out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31m_RemoteTraceback\u001B[0m                          Traceback (most recent call last)",
      "\u001B[1;31m_RemoteTraceback\u001B[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\zhong\\.conda\\envs\\climada_env\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 463, in _process_worker\n    r = call_item()\n  File \"C:\\Users\\zhong\\.conda\\envs\\climada_env\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"C:\\Users\\zhong\\.conda\\envs\\climada_env\\lib\\site-packages\\joblib\\parallel.py\", line 598, in __call__\n    return [func(*args, **kwargs)\n  File \"C:\\Users\\zhong\\.conda\\envs\\climada_env\\lib\\site-packages\\joblib\\parallel.py\", line 598, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"C:\\Users\\zhong\\.conda\\envs\\climada_env\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 129, in __call__\n    return self.function(*args, **kwargs)\n  File \"C:\\Users\\zhong\\.conda\\envs\\climada_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 887, in _fit_and_score\n    X_train, y_train = _safe_split(estimator, X, y, train)\n  File \"C:\\Users\\zhong\\.conda\\envs\\climada_env\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\", line 158, in _safe_split\n    X_subset = _safe_indexing(X, indices)\n  File \"C:\\Users\\zhong\\.conda\\envs\\climada_env\\lib\\site-packages\\sklearn\\utils\\__init__.py\", line 407, in _safe_indexing\n    return _pandas_indexing(X, indices, indices_dtype, axis=axis)\n  File \"C:\\Users\\zhong\\.conda\\envs\\climada_env\\lib\\site-packages\\sklearn\\utils\\__init__.py\", line 219, in _pandas_indexing\n    return X.take(key, axis=axis)\n  File \"C:\\Users\\zhong\\.conda\\envs\\climada_env\\lib\\site-packages\\pandas\\core\\generic.py\", line 4068, in take\n    new_data = self._mgr.take(\n  File \"C:\\Users\\zhong\\.conda\\envs\\climada_env\\lib\\site-packages\\pandas\\core\\internals\\managers.py\", line 874, in take\n    indexer = maybe_convert_indices(indexer, n, verify=verify)\n  File \"C:\\Users\\zhong\\.conda\\envs\\climada_env\\lib\\site-packages\\pandas\\core\\indexers\\utils.py\", line 282, in maybe_convert_indices\n    raise IndexError(\"indices are out-of-bounds\")\nIndexError: indices are out-of-bounds\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 23\u001B[0m\n\u001B[0;32m     20\u001B[0m gdf_all_years \u001B[38;5;241m=\u001B[39m group_into_fire_events(gdf_all_years)\n\u001B[0;32m     22\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m'''Step3: Split data into training and testing sets and train and evaluate models'''\u001B[39;00m\n\u001B[1;32m---> 23\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_and_evaluate_models\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgdf_all_years\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[6], line 60\u001B[0m, in \u001B[0;36mtrain_and_evaluate_models\u001B[1;34m(df, random_state)\u001B[0m\n\u001B[0;32m     58\u001B[0m \u001B[38;5;66;03m# Optimize hyperparameters with Optuna\u001B[39;00m\n\u001B[0;32m     59\u001B[0m study \u001B[38;5;241m=\u001B[39m optuna\u001B[38;5;241m.\u001B[39mcreate_study(sampler\u001B[38;5;241m=\u001B[39moptuna\u001B[38;5;241m.\u001B[39msamplers\u001B[38;5;241m.\u001B[39mTPESampler(), direction\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmaximize\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 60\u001B[0m \u001B[43mstudy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlogisticregression_objective\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_trials\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     61\u001B[0m study\u001B[38;5;241m.\u001B[39moptimize(xgboost_objective, n_trials\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m)\n\u001B[0;32m     63\u001B[0m results \u001B[38;5;241m=\u001B[39m {}\n",
      "File \u001B[1;32m~\\.conda\\envs\\climada_env\\lib\\site-packages\\optuna\\study\\study.py:451\u001B[0m, in \u001B[0;36mStudy.optimize\u001B[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001B[0m\n\u001B[0;32m    348\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21moptimize\u001B[39m(\n\u001B[0;32m    349\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    350\u001B[0m     func: ObjectiveFuncType,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    357\u001B[0m     show_progress_bar: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    358\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    359\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Optimize an objective function.\u001B[39;00m\n\u001B[0;32m    360\u001B[0m \n\u001B[0;32m    361\u001B[0m \u001B[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    449\u001B[0m \u001B[38;5;124;03m            If nested invocation of this method occurs.\u001B[39;00m\n\u001B[0;32m    450\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 451\u001B[0m     \u001B[43m_optimize\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    452\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstudy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    453\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfunc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    454\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_trials\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_trials\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    455\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    456\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_jobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    457\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcatch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mtuple\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43misinstance\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mIterable\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    458\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    459\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgc_after_trial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgc_after_trial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    460\u001B[0m \u001B[43m        \u001B[49m\u001B[43mshow_progress_bar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mshow_progress_bar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    461\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\climada_env\\lib\\site-packages\\optuna\\study\\_optimize.py:62\u001B[0m, in \u001B[0;36m_optimize\u001B[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001B[0m\n\u001B[0;32m     60\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     61\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m n_jobs \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m---> 62\u001B[0m         \u001B[43m_optimize_sequential\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     63\u001B[0m \u001B[43m            \u001B[49m\u001B[43mstudy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     64\u001B[0m \u001B[43m            \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     65\u001B[0m \u001B[43m            \u001B[49m\u001B[43mn_trials\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     66\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     67\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     68\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     69\u001B[0m \u001B[43m            \u001B[49m\u001B[43mgc_after_trial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     70\u001B[0m \u001B[43m            \u001B[49m\u001B[43mreseed_sampler_rng\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m     71\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtime_start\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m     72\u001B[0m \u001B[43m            \u001B[49m\u001B[43mprogress_bar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprogress_bar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     73\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     74\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     75\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m n_jobs \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m:\n",
      "File \u001B[1;32m~\\.conda\\envs\\climada_env\\lib\\site-packages\\optuna\\study\\_optimize.py:159\u001B[0m, in \u001B[0;36m_optimize_sequential\u001B[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001B[0m\n\u001B[0;32m    156\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m    158\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 159\u001B[0m     frozen_trial \u001B[38;5;241m=\u001B[39m \u001B[43m_run_trial\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstudy\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    160\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    161\u001B[0m     \u001B[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001B[39;00m\n\u001B[0;32m    162\u001B[0m     \u001B[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001B[39;00m\n\u001B[0;32m    163\u001B[0m     \u001B[38;5;66;03m# Please refer to the following PR for further details:\u001B[39;00m\n\u001B[0;32m    164\u001B[0m     \u001B[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001B[39;00m\n\u001B[0;32m    165\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m gc_after_trial:\n",
      "File \u001B[1;32m~\\.conda\\envs\\climada_env\\lib\\site-packages\\optuna\\study\\_optimize.py:247\u001B[0m, in \u001B[0;36m_run_trial\u001B[1;34m(study, func, catch)\u001B[0m\n\u001B[0;32m    240\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mShould not reach.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    242\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m    243\u001B[0m     frozen_trial\u001B[38;5;241m.\u001B[39mstate \u001B[38;5;241m==\u001B[39m TrialState\u001B[38;5;241m.\u001B[39mFAIL\n\u001B[0;32m    244\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m func_err \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    245\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(func_err, catch)\n\u001B[0;32m    246\u001B[0m ):\n\u001B[1;32m--> 247\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m func_err\n\u001B[0;32m    248\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m frozen_trial\n",
      "File \u001B[1;32m~\\.conda\\envs\\climada_env\\lib\\site-packages\\optuna\\study\\_optimize.py:196\u001B[0m, in \u001B[0;36m_run_trial\u001B[1;34m(study, func, catch)\u001B[0m\n\u001B[0;32m    194\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m get_heartbeat_thread(trial\u001B[38;5;241m.\u001B[39m_trial_id, study\u001B[38;5;241m.\u001B[39m_storage):\n\u001B[0;32m    195\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 196\u001B[0m         value_or_values \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    197\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m exceptions\u001B[38;5;241m.\u001B[39mTrialPruned \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    198\u001B[0m         \u001B[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001B[39;00m\n\u001B[0;32m    199\u001B[0m         state \u001B[38;5;241m=\u001B[39m TrialState\u001B[38;5;241m.\u001B[39mPRUNED\n",
      "Cell \u001B[1;32mIn[6], line 28\u001B[0m, in \u001B[0;36mtrain_and_evaluate_models.<locals>.logisticregression_objective\u001B[1;34m(trial)\u001B[0m\n\u001B[0;32m     26\u001B[0m classifier_obj \u001B[38;5;241m=\u001B[39m LogisticRegression(C\u001B[38;5;241m=\u001B[39mlogistic_c, random_state\u001B[38;5;241m=\u001B[39mrandom_state)\n\u001B[0;32m     27\u001B[0m model_pipeline \u001B[38;5;241m=\u001B[39m make_pipeline(StandardScaler(), classifier_obj)\n\u001B[1;32m---> 28\u001B[0m scores \u001B[38;5;241m=\u001B[39m \u001B[43mcross_val_score\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_pipeline\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_train_val\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train_val\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcv\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcustom_cv_split\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train_val\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train_val\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_splits\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msplit_by_col\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mevent_id\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrandom_state\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscoring\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mroc_auc\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     29\u001B[0m score \u001B[38;5;241m=\u001B[39m scores\u001B[38;5;241m.\u001B[39mmean()\n\u001B[0;32m     30\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m score \u001B[38;5;241m>\u001B[39m best_scores[classifier_name]:\n",
      "File \u001B[1;32m~\\.conda\\envs\\climada_env\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    207\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    208\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m    209\u001B[0m         skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m    210\u001B[0m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m    211\u001B[0m         )\n\u001B[0;32m    212\u001B[0m     ):\n\u001B[1;32m--> 213\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    214\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m InvalidParameterError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    215\u001B[0m     \u001B[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001B[39;00m\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001B[39;00m\n\u001B[0;32m    217\u001B[0m     \u001B[38;5;66;03m# the name of the estimator by the name of the function in the error\u001B[39;00m\n\u001B[0;32m    218\u001B[0m     \u001B[38;5;66;03m# message to avoid confusion.\u001B[39;00m\n\u001B[0;32m    219\u001B[0m     msg \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\n\u001B[0;32m    220\u001B[0m         \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mw+ must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    221\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    222\u001B[0m         \u001B[38;5;28mstr\u001B[39m(e),\n\u001B[0;32m    223\u001B[0m     )\n",
      "File \u001B[1;32m~\\.conda\\envs\\climada_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:719\u001B[0m, in \u001B[0;36mcross_val_score\u001B[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, error_score)\u001B[0m\n\u001B[0;32m    716\u001B[0m \u001B[38;5;66;03m# To ensure multimetric format is not supported\u001B[39;00m\n\u001B[0;32m    717\u001B[0m scorer \u001B[38;5;241m=\u001B[39m check_scoring(estimator, scoring\u001B[38;5;241m=\u001B[39mscoring)\n\u001B[1;32m--> 719\u001B[0m cv_results \u001B[38;5;241m=\u001B[39m \u001B[43mcross_validate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    720\u001B[0m \u001B[43m    \u001B[49m\u001B[43mestimator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    721\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    722\u001B[0m \u001B[43m    \u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    723\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgroups\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    724\u001B[0m \u001B[43m    \u001B[49m\u001B[43mscoring\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mscore\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mscorer\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    725\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcv\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    726\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_jobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    727\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    728\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfit_params\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfit_params\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    729\u001B[0m \u001B[43m    \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    730\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpre_dispatch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpre_dispatch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    731\u001B[0m \u001B[43m    \u001B[49m\u001B[43merror_score\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merror_score\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    732\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    733\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m cv_results[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_score\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[1;32m~\\.conda\\envs\\climada_env\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    207\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    208\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m    209\u001B[0m         skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m    210\u001B[0m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m    211\u001B[0m         )\n\u001B[0;32m    212\u001B[0m     ):\n\u001B[1;32m--> 213\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    214\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m InvalidParameterError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    215\u001B[0m     \u001B[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001B[39;00m\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001B[39;00m\n\u001B[0;32m    217\u001B[0m     \u001B[38;5;66;03m# the name of the estimator by the name of the function in the error\u001B[39;00m\n\u001B[0;32m    218\u001B[0m     \u001B[38;5;66;03m# message to avoid confusion.\u001B[39;00m\n\u001B[0;32m    219\u001B[0m     msg \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\n\u001B[0;32m    220\u001B[0m         \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mw+ must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    221\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    222\u001B[0m         \u001B[38;5;28mstr\u001B[39m(e),\n\u001B[0;32m    223\u001B[0m     )\n",
      "File \u001B[1;32m~\\.conda\\envs\\climada_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:430\u001B[0m, in \u001B[0;36mcross_validate\u001B[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001B[0m\n\u001B[0;32m    427\u001B[0m \u001B[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001B[39;00m\n\u001B[0;32m    428\u001B[0m \u001B[38;5;66;03m# independent, and that it is pickle-able.\u001B[39;00m\n\u001B[0;32m    429\u001B[0m parallel \u001B[38;5;241m=\u001B[39m Parallel(n_jobs\u001B[38;5;241m=\u001B[39mn_jobs, verbose\u001B[38;5;241m=\u001B[39mverbose, pre_dispatch\u001B[38;5;241m=\u001B[39mpre_dispatch)\n\u001B[1;32m--> 430\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[43mparallel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    431\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_fit_and_score\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    432\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclone\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    433\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    434\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    435\u001B[0m \u001B[43m        \u001B[49m\u001B[43mscorer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mscorers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    436\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrain\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    437\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtest\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    438\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    439\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparameters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    440\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfit_params\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrouted_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mestimator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    441\u001B[0m \u001B[43m        \u001B[49m\u001B[43mscore_params\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrouted_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscorer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscore\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    442\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_train_score\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_train_score\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    443\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_times\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    444\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_estimator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_estimator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    445\u001B[0m \u001B[43m        \u001B[49m\u001B[43merror_score\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merror_score\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    446\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    447\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mindices\u001B[49m\n\u001B[0;32m    448\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    450\u001B[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001B[0;32m    452\u001B[0m \u001B[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001B[39;00m\n\u001B[0;32m    453\u001B[0m \u001B[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001B[39;00m\n\u001B[0;32m    454\u001B[0m \u001B[38;5;66;03m# the correct key.\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\climada_env\\lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m     62\u001B[0m config \u001B[38;5;241m=\u001B[39m get_config()\n\u001B[0;32m     63\u001B[0m iterable_with_config \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m     64\u001B[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[0;32m     65\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[0;32m     66\u001B[0m )\n\u001B[1;32m---> 67\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterable_with_config\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\climada_env\\lib\\site-packages\\joblib\\parallel.py:2007\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   2001\u001B[0m \u001B[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001B[39;00m\n\u001B[0;32m   2002\u001B[0m \u001B[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001B[39;00m\n\u001B[0;32m   2003\u001B[0m \u001B[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001B[39;00m\n\u001B[0;32m   2004\u001B[0m \u001B[38;5;66;03m# dispatch of the tasks to the workers.\u001B[39;00m\n\u001B[0;32m   2005\u001B[0m \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[1;32m-> 2007\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\climada_env\\lib\\site-packages\\joblib\\parallel.py:1650\u001B[0m, in \u001B[0;36mParallel._get_outputs\u001B[1;34m(self, iterator, pre_dispatch)\u001B[0m\n\u001B[0;32m   1647\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m\n\u001B[0;32m   1649\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend\u001B[38;5;241m.\u001B[39mretrieval_context():\n\u001B[1;32m-> 1650\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_retrieve()\n\u001B[0;32m   1652\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mGeneratorExit\u001B[39;00m:\n\u001B[0;32m   1653\u001B[0m     \u001B[38;5;66;03m# The generator has been garbage collected before being fully\u001B[39;00m\n\u001B[0;32m   1654\u001B[0m     \u001B[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001B[39;00m\n\u001B[0;32m   1655\u001B[0m     \u001B[38;5;66;03m# the user if necessary.\u001B[39;00m\n\u001B[0;32m   1656\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\climada_env\\lib\\site-packages\\joblib\\parallel.py:1754\u001B[0m, in \u001B[0;36mParallel._retrieve\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1747\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_wait_retrieval():\n\u001B[0;32m   1748\u001B[0m \n\u001B[0;32m   1749\u001B[0m     \u001B[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m     \u001B[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001B[39;00m\n\u001B[0;32m   1751\u001B[0m     \u001B[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001B[39;00m\n\u001B[0;32m   1752\u001B[0m     \u001B[38;5;66;03m# worker traceback.\u001B[39;00m\n\u001B[0;32m   1753\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_aborting:\n\u001B[1;32m-> 1754\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_raise_error_fast\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1755\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m   1757\u001B[0m     \u001B[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001B[39;00m\n\u001B[0;32m   1758\u001B[0m     \u001B[38;5;66;03m# async callbacks to progress.\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\climada_env\\lib\\site-packages\\joblib\\parallel.py:1789\u001B[0m, in \u001B[0;36mParallel._raise_error_fast\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1785\u001B[0m \u001B[38;5;66;03m# If this error job exists, immediately raise the error by\u001B[39;00m\n\u001B[0;32m   1786\u001B[0m \u001B[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001B[39;00m\n\u001B[0;32m   1787\u001B[0m \u001B[38;5;66;03m# called directly or if the generator is gc'ed.\u001B[39;00m\n\u001B[0;32m   1788\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m error_job \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 1789\u001B[0m     \u001B[43merror_job\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_result\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\climada_env\\lib\\site-packages\\joblib\\parallel.py:745\u001B[0m, in \u001B[0;36mBatchCompletionCallBack.get_result\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    739\u001B[0m backend \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparallel\u001B[38;5;241m.\u001B[39m_backend\n\u001B[0;32m    741\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m backend\u001B[38;5;241m.\u001B[39msupports_retrieve_callback:\n\u001B[0;32m    742\u001B[0m     \u001B[38;5;66;03m# We assume that the result has already been retrieved by the\u001B[39;00m\n\u001B[0;32m    743\u001B[0m     \u001B[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001B[39;00m\n\u001B[0;32m    744\u001B[0m     \u001B[38;5;66;03m# be returned.\u001B[39;00m\n\u001B[1;32m--> 745\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_return_or_raise\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    747\u001B[0m \u001B[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001B[39;00m\n\u001B[0;32m    748\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32m~\\.conda\\envs\\climada_env\\lib\\site-packages\\joblib\\parallel.py:763\u001B[0m, in \u001B[0;36mBatchCompletionCallBack._return_or_raise\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    761\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    762\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstatus \u001B[38;5;241m==\u001B[39m TASK_ERROR:\n\u001B[1;32m--> 763\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_result\n\u001B[0;32m    764\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_result\n\u001B[0;32m    765\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
      "\u001B[1;31mIndexError\u001B[0m: indices are out-of-bounds"
     ]
    }
   ],
   "source": [
    "years = np.arange(2013, 2014)\n",
    "\n",
    "# Initialize an empty DataFrame to store the concatenated DataFrames\n",
    "gdf_all_years = pd.DataFrame()\n",
    "\n",
    "for year in years:\n",
    "    '''Step1: Load gdf'''\n",
    "    # construct the file path\n",
    "    file_path = os.path.join(folder, str(year), f'ignited_eu_{year}_gdf')\n",
    "    \n",
    "    # Load the DataFrame from the file\n",
    "    gdf = gpd.read_file(file_path)\n",
    "    \n",
    "    # Concatenate the loaded DataFrame with the initial DataFrame\n",
    "    gdf_all_years = pd.concat([gdf_all_years, gdf])\n",
    "    \n",
    "    gdf_all_years = gpd.GeoDataFrame(gdf_all_years, geometry=gdf_all_years.geometry, crs=gdf_all_years.crs)\n",
    "    \n",
    "    '''Step2: Group gdf into fire events'''\n",
    "    gdf_all_years = group_into_fire_events(gdf_all_years)\n",
    "    \n",
    "    '''Step3: Split data into training and testing sets and train and evaluate models'''\n",
    "    results = train_and_evaluate_models(gdf_all_years)\n",
    "    \n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-25T02:14:57.056820Z",
     "start_time": "2024-07-25T02:14:43.489612Z"
    }
   },
   "id": "2ef2daf7bffcecdc",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for LogisticRegression:\n",
      "Training Accuracy: 0.7700218833679412\n",
      "Test Accuracy: 0.7675706285786195\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.76      0.85      0.80      8629\n",
      "        True       0.79      0.67      0.73      7264\n",
      "\n",
      "    accuracy                           0.77     15893\n",
      "   macro avg       0.77      0.76      0.76     15893\n",
      "weighted avg       0.77      0.77      0.77     15893\n",
      "\n",
      "Confusion Matrix:\n",
      " [[7297 1332]\n",
      " [2362 4902]]\n",
      "Feature Importance:\n",
      "            feature  importance\n",
      "0              fwi    0.199789\n",
      "4    land_cover_40    0.141254\n",
      "5    land_cover_50    0.112411\n",
      "9    land_cover_90    0.023947\n",
      "19  land_cover_126    0.019786\n",
      "2    land_cover_20    0.016416\n",
      "18  land_cover_125    0.008038\n",
      "12  land_cover_112    0.000000\n",
      "3    land_cover_30   -0.000506\n",
      "17  land_cover_124   -0.001221\n",
      "15  land_cover_116   -0.015308\n",
      "16  land_cover_121   -0.019716\n",
      "10  land_cover_100   -0.021827\n",
      "7    land_cover_70   -0.039546\n",
      "1        elevation   -0.041419\n",
      "13  land_cover_114   -0.041802\n",
      "14  land_cover_115   -0.044757\n",
      "6    land_cover_60   -0.046661\n",
      "21       month_sin   -0.047801\n",
      "8    land_cover_80   -0.049809\n",
      "11  land_cover_111   -0.100353\n",
      "22       month_cos   -0.106716\n",
      "20  land_cover_200   -0.151478\n",
      "------------------------------------\n",
      "Results for XGBClassifier:\n",
      "Training Accuracy: 0.826729869748516\n",
      "Test Accuracy: 0.8235701252123576\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.91      0.75      0.82      8629\n",
      "        True       0.76      0.91      0.82      7264\n",
      "\n",
      "    accuracy                           0.82     15893\n",
      "   macro avg       0.83      0.83      0.82     15893\n",
      "weighted avg       0.84      0.82      0.82     15893\n",
      "\n",
      "Confusion Matrix:\n",
      " [[6496 2133]\n",
      " [ 671 6593]]\n",
      "Feature Importance:\n",
      "            feature  importance\n",
      "0              fwi    0.274153\n",
      "20  land_cover_200    0.152107\n",
      "4    land_cover_40    0.145927\n",
      "5    land_cover_50    0.101896\n",
      "11  land_cover_111    0.086640\n",
      "6    land_cover_60    0.054461\n",
      "22       month_cos    0.048469\n",
      "19  land_cover_126    0.047128\n",
      "21       month_sin    0.044218\n",
      "9    land_cover_90    0.023590\n",
      "1        elevation    0.021409\n",
      "16  land_cover_121    0.000000\n",
      "2    land_cover_20    0.000000\n",
      "18  land_cover_125    0.000000\n",
      "17  land_cover_124    0.000000\n",
      "3    land_cover_30    0.000000\n",
      "15  land_cover_116    0.000000\n",
      "13  land_cover_114    0.000000\n",
      "12  land_cover_112    0.000000\n",
      "10  land_cover_100    0.000000\n",
      "8    land_cover_80    0.000000\n",
      "7    land_cover_70    0.000000\n",
      "14  land_cover_115    0.000000\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for model, metrics in results.items():\n",
    "    print(f\"Results for {model}:\")\n",
    "    print(\"Training Accuracy:\", metrics['train_accuracy'])\n",
    "    print(\"Test Accuracy:\", metrics['test_accuracy'])\n",
    "    print(\"Classification Report:\\n\", metrics['classification_report'])\n",
    "    print(\"Confusion Matrix:\\n\", metrics['confusion_matrix'])\n",
    "    print(\"Feature Importance:\\n\", metrics['feature_importance'])\n",
    "    print('------------------------------------')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-23T03:43:47.635927Z",
     "start_time": "2024-07-23T03:43:47.627927Z"
    }
   },
   "id": "9c41cbde338d1cad",
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6c1e54f70e2a52da"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
