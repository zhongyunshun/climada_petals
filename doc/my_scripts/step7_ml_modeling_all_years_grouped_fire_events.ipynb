{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-25T02:13:33.965600Z",
     "start_time": "2024-07-25T02:13:33.748601Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Load relevant functions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9012436159c5902f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.1 Load the function to group the DataFrame into fire events"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "29b3b14f360028ce"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def group_into_fire_events(df):\n",
    "    \"\"\"\n",
    "    This function takes in a DataFrame and returns a copy of the DataFrame with three additional columns:\n",
    "    'date_cluster', 'regional_cluster', and 'event_id'.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        df (pd.DataFrame): The output DataFrame with three additional columns: 'date_cluster', 'regional_cluster', and 'event_id'.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.copy()\n",
    "    df.rename(columns={'latitude_left': 'latitude', 'longitude_left': 'longitude'}, inplace=True)\n",
    "    \n",
    "    # Convert 'acq_date' to datetime\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    df = df.sort_values(by='date')\n",
    "    \n",
    "    # Create 'date_cluster' column based on consecutive dates\n",
    "    df['date_cluster'] = (df['date'].diff().dt.days > 1).cumsum()\n",
    "    \n",
    "    # Function to apply DBSCAN on each consecutive date cluster\n",
    "    def apply_dbscan(group):\n",
    "        coords = group[['latitude', 'longitude']].values \n",
    "        # Apply DBSCAN, hyperparameters are the same as Climada\n",
    "        db = DBSCAN(eps=15/111.12, min_samples=1).fit(coords)\n",
    "        group['regional_cluster'] = db.labels_\n",
    "        return group\n",
    "    \n",
    "    # Apply DBSCAN on each date cluster\n",
    "    df = df.groupby('date_cluster').apply(apply_dbscan)\n",
    "    \n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Create a unique 'event_id' for each unique combination of 'date_cluster' and 'regional_cluster'\n",
    "    df['event_id'] = df.groupby(['date_cluster', 'regional_cluster']).ngroup()\n",
    "    \n",
    "    df = df.sort_values(by='event_id').reset_index(drop=True)\n",
    "    \n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-25T02:13:33.980600Z",
     "start_time": "2024-07-25T02:13:33.966601Z"
    }
   },
   "id": "e671d22827bb58f1",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 Load the function to shuffle and split data into training and testing according to event_id"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "24844bc42599e48a"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def split_event_ids(event_id_pairs, test_size, random_seed):\n",
    "    \"\"\"\n",
    "    Splits event_id pairs into training and test sets based on the specified test size.\n",
    "\n",
    "    Args:\n",
    "        event_id_pairs (list of tuples): List of event_id and their corresponding row counts.\n",
    "        test_size (float): The proportion of the test set.\n",
    "        random_seed (int): The seed for the random number generator.\n",
    "\n",
    "    Returns:\n",
    "        train_event_ids (list): List of event_ids for the training set.\n",
    "        test_event_ids (list): List of event_ids for the test set.\n",
    "    \"\"\"\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(event_id_pairs)\n",
    "    \n",
    "    test_event_ids = []\n",
    "    current_test_rows = 0\n",
    "    total_rows = sum(count for _, count in event_id_pairs)\n",
    "    test_rows_target = int(total_rows * test_size)\n",
    "    \n",
    "    for event_id, count in event_id_pairs:\n",
    "        current_test_rows += count\n",
    "        test_event_ids.append(event_id)\n",
    "        if current_test_rows >= test_rows_target:\n",
    "            break\n",
    "    \n",
    "    train_event_ids = [event_id for event_id, _ in event_id_pairs if event_id not in test_event_ids]\n",
    "    \n",
    "    return train_event_ids, test_event_ids\n",
    "\n",
    "def shuffle_and_split(df, test_size=0.1, random_seed=42):\n",
    "    \"\"\"\n",
    "    This function shuffles the DataFrame with respect to 'event_id' and then splits the data\n",
    "    into training and test sets with approximately 10% test size with respect to 'event_id'.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame\n",
    "        test_size (float): The proportion of the test set\n",
    "        random_seed (int): The seed for the random number generator\n",
    "        \n",
    "    Returns:\n",
    "        X_train (pd.DataFrame): Training features\n",
    "        X_test (pd.DataFrame): Test features\n",
    "        y_train (pd.Series): Training labels\n",
    "        y_test (pd.Series): Test labels\n",
    "    \"\"\"\n",
    "    # Shuffle the dataframe based on event_id\n",
    "    event_counts = df['event_id'].value_counts().sort_index()\n",
    "    event_id_pairs = list(zip(event_counts.index, event_counts.values))\n",
    "    \n",
    "    train_event_ids, test_event_ids = split_event_ids(event_id_pairs, test_size, random_seed)\n",
    "    \n",
    "    X_train = df[df['event_id'].isin(train_event_ids)].drop(columns=['ignited'])\n",
    "    y_train = df[df['event_id'].isin(train_event_ids)]['ignited']\n",
    "    X_test = df[df['event_id'].isin(test_event_ids)].drop(columns=['ignited'])\n",
    "    y_test = df[df['event_id'].isin(test_event_ids)]['ignited']\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# folder = '../../climada_petals/data/wildfire/outputs/'\n",
    "# year = 2013\n",
    "# file_path = os.path.join(folder, str(year), f'ignited_eu_{year}_gdf')\n",
    "#     \n",
    "# # Load the DataFrame from the file\n",
    "# df = gpd.read_file(file_path)\n",
    "# X_train, X_test, y_train, y_test = shuffle_and_split(df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-25T02:19:24.575958Z",
     "start_time": "2024-07-25T02:17:19.409847Z"
    }
   },
   "id": "54ca58bd7ec20886",
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c9c8a89097f6579"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.3 Load the function for cross validation split according to specified column (e.g., 'event_id')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "85144ffc6773a875"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def custom_cv_split(X, y, n_splits=5, split_by_col='event_id', random_state=42):\n",
    "    \"\"\"\n",
    "    This function creates cross-validation splits based on a specified column (e.g., 'event_id'),\n",
    "    ensuring that all rows with the same value in the specified column are kept together in the same fold.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): Features\n",
    "        y (pd.Series): Labels\n",
    "        n_splits (int): Number of folds\n",
    "        split_by_col (str): Column name to split by (e.g., 'event_id')\n",
    "        random_state (int): Random seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "        splits (list): List of tuples containing train and validation indices for each fold\n",
    "    \"\"\"\n",
    "    # Ensure reproducibility\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Get unique event IDs and shuffle them\n",
    "    unique_event_ids = X[split_by_col].unique()\n",
    "    np.random.shuffle(unique_event_ids)\n",
    "    \n",
    "    # Calculate the fold sizes\n",
    "    fold_sizes = np.full(n_splits, X.shape[0] // n_splits, dtype=int)\n",
    "    fold_sizes[: X.shape[0] % n_splits] += 1\n",
    "    \n",
    "    splits = []\n",
    "    current = 0\n",
    "    for fold_size in fold_sizes:\n",
    "        val_event_ids = []\n",
    "        current_fold_size = 0\n",
    "        # Accumulate event IDs until the fold size limit is reached\n",
    "        while current_fold_size < fold_size and current < len(unique_event_ids):\n",
    "            event_id = unique_event_ids[current]\n",
    "            event_size = len(X[X[split_by_col] == event_id])\n",
    "            # Break if adding this event would exceed the fld size\n",
    "            if current_fold_size + event_size > fold_size:\n",
    "                break\n",
    "            val_event_ids.append(event_id)\n",
    "            current_fold_size += event_size\n",
    "            current += 1\n",
    "            \n",
    "        # Get training event IDs by excluding validation event IDs\n",
    "        train_event_ids = np.setdiff1d(unique_event_ids, val_event_ids)\n",
    "        \n",
    "        # Get the indices for training and validation sets\n",
    "        train_indices = X[X[split_by_col].isin(train_event_ids)].index\n",
    "        val_indices = X[X[split_by_col].isin(val_event_ids)].index\n",
    "        \n",
    "        # Append the indices for the current fold to the list\n",
    "        splits.append((train_indices, val_indices))\n",
    "        \n",
    "    return splits\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-25T02:13:35.809539Z",
     "start_time": "2024-07-25T02:13:35.805193Z"
    }
   },
   "id": "2d2bf4826e354f03",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1.4 Load the function to train and evaluate ML models"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e155bdf92dde1424"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_and_evaluate_models(df, random_state=42):\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # Split the data\n",
    "    X_train_val, X_test, y_train_val, y_test = shuffle_and_split(df, test_size=0.1, random_seed=random_state)\n",
    "    # Without resetting the index, index in custom_cv_split is not working and will get index-out-of-bound error\n",
    "    X_train_val.reset_index(drop=True, inplace=True)\n",
    "    X_test.reset_index(drop=True, inplace=True)\n",
    "    y_train_val = y_train_val.reset_index(drop=True)\n",
    "    y_test = y_test.reset_index(drop=True)\n",
    "    \n",
    "    X_train_val = X_train_val.drop(columns=['latitude', 'longitude', 'brightness', 'bright_t31', 'month', 'date', 'distance_km', 'confidence', 'geometry'])\n",
    "    X_test = X_test.drop(columns=['latitude', 'longitude', 'brightness', 'bright_t31', 'month', 'date', 'distance_km', 'confidence', 'geometry'])\n",
    "\n",
    "    # Initialize dictionaries to store the best models, hyperparameters, and scores\n",
    "    classifier_names = [\"LogisticRegression\", \"XGBClassifier\"]\n",
    "    best_models = {name: None for name in classifier_names}\n",
    "    best_params = {name: None for name in classifier_names}\n",
    "    best_scores = {name: 0 for name in classifier_names}\n",
    "\n",
    "    # Define objective functions for Optuna\n",
    "    def logisticregression_objective(trial):\n",
    "        classifier_name = \"LogisticRegression\"\n",
    "        logistic_c = trial.suggest_float('logistic_c', 1e-5, 1e5, log=True)\n",
    "        classifier_obj = LogisticRegression(C=logistic_c, random_state=random_state)\n",
    "        model_pipeline = make_pipeline(StandardScaler(), classifier_obj)\n",
    "        cv_splits = custom_cv_split(X_train_val, y_train_val, n_splits=5, split_by_col='event_id', random_state=random_state)\n",
    "        scores = cross_val_score(model_pipeline, X_train_val, y_train_val, cv=cv_splits, n_jobs=-1, scoring='roc_auc')\n",
    "        score = scores.mean()\n",
    "        if score > best_scores[classifier_name]:\n",
    "            best_scores[classifier_name] = score\n",
    "            best_params[classifier_name] = trial.params\n",
    "            best_models[classifier_name] = classifier_obj # classifier_obj remain untrained\n",
    "        return score\n",
    "\n",
    "    def xgboost_objective(trial):\n",
    "        classifier_name = \"XGBClassifier\"\n",
    "        xgb_params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 200, 2000),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 1e-3, 1e-1, log=True),\n",
    "            'subsample': trial.suggest_float('subsample', 0.2, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "            'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 0, 1),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 0, 1),\n",
    "            'random_state': random_state\n",
    "        }\n",
    "        classifier_obj = XGBClassifier(**xgb_params)\n",
    "        cv_splits = custom_cv_split(X_train_val, y_train_val, n_splits=5, split_by_col='event_id', random_state=random_state)\n",
    "        scores = cross_val_score(model_pipeline, X_train_val, y_train_val, cv=cv_splits, n_jobs=-1, scoring='roc_auc')\n",
    "        score = scores.mean()\n",
    "        if score > best_scores[classifier_name]:\n",
    "            best_scores[classifier_name] = score\n",
    "            best_params[classifier_name] = trial.params\n",
    "            best_models[classifier_name] = classifier_obj # classifier_obj remain untrained\n",
    "        return score\n",
    "\n",
    "    # Optimize hyperparameters with Optuna\n",
    "    study = optuna.create_study(sampler=optuna.samplers.TPESampler(), direction='maximize')\n",
    "    study.optimize(logisticregression_objective, n_trials=20)\n",
    "    study.optimize(xgboost_objective, n_trials=100)\n",
    "\n",
    "    results = {}\n",
    "    for name in classifier_names:\n",
    "        if name == \"LogisticRegression\":\n",
    "            best_models[name].set_params(max_iter=200)\n",
    "            model_pipeline = make_pipeline(StandardScaler(), best_models[name])\n",
    "        else:\n",
    "            model_pipeline = best_models[name]\n",
    "\n",
    "        model_pipeline.fit(X_train_val, y_train_val)\n",
    "        train_accuracy = model_pipeline.score(X_train_val, y_train_val)\n",
    "        test_accuracy = model_pipeline.score(X_test, y_test)\n",
    "        y_pred = model_pipeline.predict(X_test)\n",
    "        classification_rep = classification_report(y_test, y_pred)\n",
    "        confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "        results[name] = {\n",
    "            \"train_roc_auc\": train_accuracy,\n",
    "            \"test_roc_auc\": test_accuracy,\n",
    "            \"classification_report\": classification_rep,\n",
    "            \"confusion_matrix\": confusion_mat\n",
    "        }\n",
    "\n",
    "        if name == \"LogisticRegression\":\n",
    "            feature_importance = best_models[name].coef_[0]\n",
    "        else:\n",
    "            feature_importance = best_models[name].feature_importances_\n",
    "\n",
    "        feature_importance_df = pd.DataFrame({'feature': X_train_val.columns, 'importance': feature_importance})\n",
    "        feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)\n",
    "        results[name][\"feature_importance\"] = feature_importance_df\n",
    "\n",
    "    return results\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-25T02:13:38.162225Z",
     "start_time": "2024-07-25T02:13:38.142225Z"
    }
   },
   "id": "c7cf786755d721",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1.5 Load the function to train and evaluate ML models (Min 1s interval per API call)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1479eec173d7a52a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "# Initialize Nominatim API\n",
    "geolocator = Nominatim(user_agent=\"geoapiExercises\")\n",
    "\n",
    "def get_country(row):\n",
    "    # Get the location from the latitude and longitude\n",
    "    location = geolocator.reverse((row['latitude'], row['longitude']))\n",
    "    address = location.raw['address']\n",
    "    # Get the country\n",
    "    country = address.get('country', '')\n",
    "    return country"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "93cd335f66215e17"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Max Consecutive days for each year"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e0a0fabe0e9b4c6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "folder = '../../climada_petals/data/wildfire/outputs/'\n",
    "years = np.arange(2001, 2024)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "688a8875045d081c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def find_max_consecutive_ones(nums) -> int:\n",
    "    \"\"\"\n",
    "    This function takes in a list of binary values and returns the maximum number of consecutive ones.\n",
    "    \n",
    "    Args:\n",
    "        nums (List[float]): The input list of float values\n",
    "        \n",
    "    Returns:\n",
    "        int: The maximum number of consecutive ones\n",
    "    \"\"\"\n",
    "    max_consecutive_ones = 0\n",
    "    current_consecutive_ones = 0\n",
    "    \n",
    "    for num in nums:\n",
    "        if num == 1.0:\n",
    "            current_consecutive_ones += 1\n",
    "        else:\n",
    "            current_consecutive_ones = 0\n",
    "        if current_consecutive_ones > max_consecutive_ones:\n",
    "            max_consecutive_ones = current_consecutive_ones \n",
    "    return max_consecutive_ones\n",
    "\n",
    "def max_consecutive_dates(group):\n",
    "    # Remove duplicates dates\n",
    "    group = group.drop_duplicates(subset='date')\n",
    "    # Sort the DataFrame by date\n",
    "    group = group.sort_values(by='date')\n",
    "    # Calculate difference in days between consecutive days\n",
    "    group['date_diff'] = group['date'].diff().dt.days.fillna(0)\n",
    "    # Find the maximum number of consecutive days\n",
    "    max_consecutive_days = find_max_consecutive_ones(group['date_diff'].values)\n",
    "    return max_consecutive_days"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa5a1af80885c54d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def select_region(df, bound):\n",
    "    \"\"\"\n",
    "    Select the region of interest from the dataframe.\n",
    "    df: hazard dataframe\n",
    "    bound: tuple in format (lon_min, lat_min, lon_max, lat_max)\n",
    "    \"\"\"\n",
    "    return df[(df['longitude'] >= bound[0]) & \n",
    "              (df['latitude'] >= bound[1]) & \n",
    "              (df['longitude'] <= bound[2]) & \n",
    "              (df['latitude'] <= bound[3])]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "288445ca268d54e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "max_consecutive_days_all_years = []\n",
    "for year in years:\n",
    "    # construct the file path\n",
    "    file_path = os.path.join(folder, str(year), f'ignited_eu_{year}_gdf')\n",
    "    \n",
    "    # Load the DataFrame from the file\n",
    "    df = gpd.read_file(file_path)\n",
    "    \n",
    "    geo_bound_uk = (-10, 49, 2, 61)\n",
    "    \n",
    "    df = select_region(df, geo_bound_uk)\n",
    "    \n",
    "    df = group_into_fire_events(df)\n",
    "    \n",
    "    # Calculate the maximum number of consecutive days\n",
    "    max_consecutive_days = df.groupby('event_id').apply(max_consecutive_dates).max()\n",
    "    max_consecutive_days_all_years.append(max_consecutive_days)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b15856f77631c69"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Plot the maximum number of consecutive days for each year\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(max_consecutive_days_all_years, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Max Consecutive Days')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Maximum Consecutive Days (2001-2024)')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "389c4120b9f2c385"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Run ML model for each year"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fd5579c337435486"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "folder = '../../climada_petals/data/wildfire/outputs/'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-25T02:14:40.539791Z",
     "start_time": "2024-07-25T02:14:40.534791Z"
    }
   },
   "id": "78ebdf6130b89ba4",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists(folder)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-25T02:14:40.967844Z",
     "start_time": "2024-07-25T02:14:40.950845Z"
    }
   },
   "id": "3f63eede1e53e28b",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def preprocess_gdf(df):\n",
    "    # drop rows without fwi values\n",
    "    df.dropna(subset=['fwi'], inplace=True)\n",
    "    # drop rows that the land type is sea\n",
    "    df = df[df['land_cover'] != 200]\n",
    "    \n",
    "    # balance data\n",
    "    df_true = df[df['ignited'] == True]\n",
    "    df_false = df[df['ignited'] == False]\n",
    "    \n",
    "    df_false_downsampled = df_false.sample(n=min(df_true.shape[0], df_false.shape[0]), random_state=42, replace=False)\n",
    "    df_true_downsampled = df_true.sample(n=min(df_true.shape[0], df_false.shape[0]), random_state=42, replace=False)\n",
    "    \n",
    "    df = pd.concat([df_true_downsampled, df_false_downsampled])\n",
    "    \n",
    "    # Preprocess the dataframe\n",
    "    df = pd.get_dummies(df, columns=['land_cover'], prefix='land_cover')\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    \n",
    "    return df\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77eb30d4c1754395"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "years = np.arange(2001, 2024)\n",
    "geo_bound_uk = (-10, 49, 2, 61)\n",
    "\n",
    "# Initialize an empty DataFrame to store the concatenated DataFrames\n",
    "gdf_all_years = pd.DataFrame()\n",
    "results = {}\n",
    "\n",
    "for year in years:\n",
    "    # construct the file path\n",
    "    file_path = os.path.join(folder, str(year), f'ignited_eu_{year}_gdf')\n",
    "\n",
    "    # Load the DataFrame from the file\n",
    "    gdf = gpd.read_file(file_path)\n",
    "\n",
    "    # Select the region of interest\n",
    "    gdf = select_region(gdf, geo_bound_uk)\n",
    "\n",
    "    # Concatenate the loaded DataFrame with the initial DataFrame\n",
    "    gdf_all_years = pd.concat([gdf_all_years, gdf])\n",
    "\n",
    "    # Group gdf into fire events\n",
    "    gdf_all_years = group_into_fire_events(gdf_all_years)\n",
    "\n",
    "    # Split data into training and testing sets and train and evaluate models\n",
    "    result_one_year = train_and_evaluate_models(gdf_all_years)\n",
    "    results[year] = result_one_year\n",
    "    print(f\"Finished training and evaluating models for {year}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-30T21:47:23.116187Z",
     "start_time": "2024-07-30T21:47:23.101184Z"
    }
   },
   "id": "2ef2daf7bffcecdc",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for LogisticRegression:\n",
      "Training Accuracy: 0.7700218833679412\n",
      "Test Accuracy: 0.7675706285786195\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.76      0.85      0.80      8629\n",
      "        True       0.79      0.67      0.73      7264\n",
      "\n",
      "    accuracy                           0.77     15893\n",
      "   macro avg       0.77      0.76      0.76     15893\n",
      "weighted avg       0.77      0.77      0.77     15893\n",
      "\n",
      "Confusion Matrix:\n",
      " [[7297 1332]\n",
      " [2362 4902]]\n",
      "Feature Importance:\n",
      "            feature  importance\n",
      "0              fwi    0.199789\n",
      "4    land_cover_40    0.141254\n",
      "5    land_cover_50    0.112411\n",
      "9    land_cover_90    0.023947\n",
      "19  land_cover_126    0.019786\n",
      "2    land_cover_20    0.016416\n",
      "18  land_cover_125    0.008038\n",
      "12  land_cover_112    0.000000\n",
      "3    land_cover_30   -0.000506\n",
      "17  land_cover_124   -0.001221\n",
      "15  land_cover_116   -0.015308\n",
      "16  land_cover_121   -0.019716\n",
      "10  land_cover_100   -0.021827\n",
      "7    land_cover_70   -0.039546\n",
      "1        elevation   -0.041419\n",
      "13  land_cover_114   -0.041802\n",
      "14  land_cover_115   -0.044757\n",
      "6    land_cover_60   -0.046661\n",
      "21       month_sin   -0.047801\n",
      "8    land_cover_80   -0.049809\n",
      "11  land_cover_111   -0.100353\n",
      "22       month_cos   -0.106716\n",
      "20  land_cover_200   -0.151478\n",
      "------------------------------------\n",
      "Results for XGBClassifier:\n",
      "Training Accuracy: 0.826729869748516\n",
      "Test Accuracy: 0.8235701252123576\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.91      0.75      0.82      8629\n",
      "        True       0.76      0.91      0.82      7264\n",
      "\n",
      "    accuracy                           0.82     15893\n",
      "   macro avg       0.83      0.83      0.82     15893\n",
      "weighted avg       0.84      0.82      0.82     15893\n",
      "\n",
      "Confusion Matrix:\n",
      " [[6496 2133]\n",
      " [ 671 6593]]\n",
      "Feature Importance:\n",
      "            feature  importance\n",
      "0              fwi    0.274153\n",
      "20  land_cover_200    0.152107\n",
      "4    land_cover_40    0.145927\n",
      "5    land_cover_50    0.101896\n",
      "11  land_cover_111    0.086640\n",
      "6    land_cover_60    0.054461\n",
      "22       month_cos    0.048469\n",
      "19  land_cover_126    0.047128\n",
      "21       month_sin    0.044218\n",
      "9    land_cover_90    0.023590\n",
      "1        elevation    0.021409\n",
      "16  land_cover_121    0.000000\n",
      "2    land_cover_20    0.000000\n",
      "18  land_cover_125    0.000000\n",
      "17  land_cover_124    0.000000\n",
      "3    land_cover_30    0.000000\n",
      "15  land_cover_116    0.000000\n",
      "13  land_cover_114    0.000000\n",
      "12  land_cover_112    0.000000\n",
      "10  land_cover_100    0.000000\n",
      "8    land_cover_80    0.000000\n",
      "7    land_cover_70    0.000000\n",
      "14  land_cover_115    0.000000\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for year in years:\n",
    "    for model, metrics in results[year].items():\n",
    "        print(f\"Results for {model}:\")\n",
    "        print(\"Training ROC AUC:\", metrics['train_roc_auc'])\n",
    "        print(\"Test ROC AUC:\", metrics['test_roc_auc'])\n",
    "        print(\"Classification Report:\\n\", metrics['classification_report'])\n",
    "        print(\"Confusion Matrix:\\n\", metrics['confusion_matrix'])\n",
    "        print(\"Feature Importance:\\n\", metrics['feature_importance'])\n",
    "        print('------------------------------------')\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-23T03:43:47.635927Z",
     "start_time": "2024-07-23T03:43:47.627927Z"
    }
   },
   "id": "9c41cbde338d1cad",
   "execution_count": 44
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Visualization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f95f3f786be6925"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e1360eb160dcd68e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Extracting data for plotting\n",
    "data = []\n",
    "for year in years:\n",
    "    for model, metrics in results[year].items():\n",
    "        data.append({'year': year, 'model': model, 'train_roc_auc': metrics['train_roc_auc'], 'test_roc_auc': metrics['test_roc_auc']})\n",
    "\n",
    "# Creating dataframe\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "'''ROC AUC over years for LogisticRegression and XGBClassifier'''\n",
    "\n",
    "# Plotting with y-axis range set from 0.85 to 1\n",
    "plt.figure(figsize=(18, 9))\n",
    "\n",
    "# Plot for LogisticRegression\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.lineplot(data=df[df['model'] == 'LogisticRegression'], x='year', y='train_roc_auc', label='Train ROC AUC', marker='o')\n",
    "sns.lineplot(data=df[df['model'] == 'LogisticRegression'], x='year', y='test_roc_auc', label='Test ROC AUC', marker='o')\n",
    "for i in range(len(df[df['model'] == 'LogisticRegression'])):\n",
    "    year = df[df['model'] == 'LogisticRegression'].iloc[i]['year']\n",
    "    train_roc_auc = df[df['model'] == 'LogisticRegression'].iloc[i]['train_roc_auc']\n",
    "    test_roc_auc = df[df['model'] == 'LogisticRegression'].iloc[i]['test_roc_auc']\n",
    "    plt.text(year, train_roc_auc, f'{train_roc_auc:.2f}', fontsize=9, ha='right')\n",
    "    plt.text(year, test_roc_auc, f'{test_roc_auc:.2f}', fontsize=9, ha='right')\n",
    "plt.title('Logistic Regression ROC AUC over Years')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('ROC AUC')\n",
    "plt.ylim(0.85, 1)\n",
    "plt.legend()\n",
    "plt.xticks(years, rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot for XGBClassifier\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.lineplot(data=df[df['model'] == 'XGBClassifier'], x='year', y='train_roc_auc', label='Train ROC AUC', marker='o')\n",
    "sns.lineplot(data=df[df['model'] == 'XGBClassifier'], x='year', y='test_roc_auc', label='Test ROC AUC', marker='o')\n",
    "for i in range(len(df[df['model'] == 'XGBClassifier'])):\n",
    "    year = df[df['model'] == 'XGBClassifier'].iloc[i]['year']\n",
    "    train_roc_auc = df[df['model'] == 'XGBClassifier'].iloc[i]['train_roc_auc']\n",
    "    test_roc_auc = df[df['model'] == 'XGBClassifier'].iloc[i]['test_roc_auc']\n",
    "    plt.text(year, train_roc_auc, f'{train_roc_auc:.2f}', fontsize=9, ha='right')\n",
    "    plt.text(year, test_roc_auc, f'{test_roc_auc:.2f}', fontsize=9, ha='right')\n",
    "plt.title('XGBClassifier ROC AUC over Years')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('ROC AUC')\n",
    "plt.ylim(0.85, 1)\n",
    "plt.legend()\n",
    "plt.xticks(years, rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f068af20a09dd93"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "'''Confusion Matrix'''\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Summing confusion matrices for each model\n",
    "logistic_conf_matrix = np.zeros((2, 2))\n",
    "xgb_conf_matrix = np.zeros((2, 2))\n",
    "\n",
    "for year in years:\n",
    "    logistic_conf_matrix += np.array(results[year]['LogisticRegression']['confusion_matrix'])\n",
    "    xgb_conf_matrix += np.array(results[year]['XGBClassifier']['confusion_matrix'])\n",
    "\n",
    "# Plotting the confusion matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# Logistic Regression Confusion Matrix\n",
    "sns.heatmap(logistic_conf_matrix, annot=True, fmt='g', ax=axes[0], cmap='Blues')\n",
    "axes[0].set_title('Logistic Regression Total Confusion Matrix (2001-2023)')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "axes[0].set_xticklabels(['Negative', 'Positive'])\n",
    "axes[0].set_yticklabels(['Negative', 'Positive'])\n",
    "\n",
    "# XGBClassifier Confusion Matrix\n",
    "sns.heatmap(xgb_conf_matrix, annot=True, fmt='g', ax=axes[1], cmap='Blues')\n",
    "axes[1].set_title('XGBClassifier Total Confusion Matrix (2001-2023)')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "axes[1].set_xticklabels(['Negative', 'Positive'])\n",
    "axes[1].set_yticklabels(['Negative', 'Positive'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f6be0a33c391f7f1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "'''Feature Importance'''\n",
    "# Aggregating feature importances for each model and getting absolute values\n",
    "logistic_feature_importance = pd.DataFrame(columns=['feature', 'importance'])\n",
    "xgb_feature_importance = pd.DataFrame(columns=['feature', 'importance'])\n",
    "\n",
    "for year in years:\n",
    "    logistic_feature_importance = pd.concat([logistic_feature_importance, results[year]['LogisticRegression']['feature_importance']], ignore_index=True)\n",
    "    xgb_feature_importance = pd.concat([xgb_feature_importance, results[year]['XGBClassifier']['feature_importance']], ignore_index=True)\n",
    "\n",
    "# Getting absolute values of importances\n",
    "logistic_feature_importance['importance'] = logistic_feature_importance['importance'].abs()\n",
    "xgb_feature_importance['importance'] = xgb_feature_importance['importance'].abs()\n",
    "\n",
    "# Averaging feature importances\n",
    "avg_logistic_feature_importance = logistic_feature_importance.groupby('feature').mean().reset_index()\n",
    "avg_xgb_feature_importance = xgb_feature_importance.groupby('feature').mean().reset_index()\n",
    "\n",
    "# Plotting\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# Logistic Regression Feature Importance\n",
    "sns.barplot(data=avg_logistic_feature_importance, x='importance', y='feature', ax=axes[0], palette='Blues_d')\n",
    "axes[0].set_title('Logistic Regression Average Feature Importance (2001-2023)')\n",
    "axes[0].set_xlabel('Importance')\n",
    "axes[0].set_ylabel('Feature')\n",
    "\n",
    "# XGBClassifier Feature Importance\n",
    "sns.barplot(data=avg_xgb_feature_importance, x='importance', y='feature', ax=axes[1], palette='Blues_d')\n",
    "axes[1].set_title('XGBClassifier Average Feature Importance (2001-2023)')\n",
    "axes[1].set_xlabel('Importance')\n",
    "axes[1].set_ylabel('Feature')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e8d8ecae2dee27c3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Save the results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8313e9438f7d9c1f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import json\n",
    "filename = 'results_all_years.json'\n",
    "file_path = os.path.join(folder, filename)\n",
    "\n",
    "# Write the results to a JSON file\n",
    "with open(file_path, 'w') as f:\n",
    "    json.dump(results, f)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c57683565be975e9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
