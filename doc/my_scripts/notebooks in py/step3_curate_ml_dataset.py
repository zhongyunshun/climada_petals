# -*- coding: utf-8 -*-
"""step3_curate_ml_dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TMl7R8IcMIm5sCZkI3emiJ2c-v2SwhhJ
"""

import os
import pandas as pd
import geopandas as gpd
import xarray as xr
import math
import rasterio
from rasterio.merge import merge
from rasterio.io import MemoryFile
import numpy as np
from rasterio.io import MemoryFile
from rasterio.windows import Window

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline

"""**Build ML models to predict fire ignition for a pixel**

Features: FWI, lat, lon, elevation (2010), landcover (2015), distance, month
Label: ignited or not ignited
Model: XGBoost
Optimization Method: Beyas optimization using Optuna
Data range: EU

Steps:
1. Merge positive and negative labels of fire intensity and FWi data
    1.1 Load left join data (fire intensity left join FWI, resolution 1km) and count how many data points it has
    1.2 Assign positive labels (ignited) to the data points with distance <= 31 * sqrt(2) / 2 and only keep rows with positive labels
    1.3 Load right join data (fire intensity right join FWI, resolution 31km) and count how may data points it has
    1.4 Down sampling the data to match the number of left join data
    1.5 Assign negative labels (ignited) to the data points with distance > 31 * sqrt(2) / 2 and only keep rows with negative labels
    1.6 Merge data and save the geo dataframe and save the dataframe
2. Add land cover (aggregated) data to pixel according to lon and lat
    2.1 Load land cover shapefile and convert to geopandas dataframe
    2.2 for each pixel (1x1 for left and 31x31 for right), calculate the average probability of fire ignition according to Clamada Master thesis Table 2.1
3. Add elevation data to pixel according to lon and lat
    3.1 Load elevation shapefile and convert to geopandas dataframe
    3.2 for each pixel (1x1 for left and 31x31 for right), calculate the average elevation
4. Save data to GeoDataFrame

Issue solved:

1. Longitude range in right joined dataset was not correct. Now correct after rerun the code.
2. If the lon and lat can't be find in the elevation data raster, it will be converted to indices outside the raster. Now it will be converted to the nearest valid elevation.

# 1. Merge positive and negative labels of fire intensity and FWi data
"""

from geopy.distance import great_circle

# Define a function to calculate the distance
def calculate_distance(row):
    coords_1 = (row['latitude_left'], row['longitude_left'])
    coords_2 = (row['latitude_right'], row['longitude_right'])
    return great_circle(coords_1, coords_2).kilometers

folder = '../../climada_petals/data/wildfire/output/2013/'

"""## 1.1 Load left join data (fire intensity left join FWI, resolution 1km)"""

merged_eu_2013_left_gdf_filename = 'merged_eu_2013_left_gdf'
df_left_join = gpd.read_file(os.path.join(folder, merged_eu_2013_left_gdf_filename))
df_left_join.shape

# Apply the function to each row in the GeoDataFrame and create a new column 'distance_km'
df_left_join['distance_km'] = df_left_join.apply(calculate_distance, axis=1)

df_left_join['distance_km'].describe()

df_left_join['longitude_left'].describe()

df_left_join

# only keep the relevant rows
df_left_join = df_left_join[['latitude_left', 'longitude_left', 'brightness', 'confidence', 'bright_t31', 'fwi', 'distance_km', 'date', 'geometry']]

"""## 1.2 Assign positive labels (ignited) to the data points with distance <= 31 * sqrt(2) / 2 and only keep rows with positive labels"""

# Drop rows with distance_km > 31 km
df_left_join = df_left_join[df_left_join['distance_km'] <= 31 * math.sqrt(2) / 2 ]
# Drop rows with confidence < 30
df_left_join = df_left_join[df_left_join['confidence'] >= 30]

df_left_join.shape

df_left_join['ignited'] = True

"""## 1.3 Load and down sampling right join data (fire intensity right join FWI, resolution 31km)"""

folder = '../../climada_petals/data/wildfire/output/2013/'
# only NetCDF works for interpolated data
file = 'merged_eu_2013_right_gdf'
df = gpd.read_file(os.path.join(folder, file))

df.shape

df['fwi'].isna().sum()

df.dropna(subset=['fwi'], inplace=True)

df['fwi'].isna().sum()

df.head()

df['longitude_right'].describe()

"""## 1.4 Down sampling the data to match the number of left join data"""

# Randomly sample 100,000 rows without replacement
df_right_join = df.sample(n=100000, replace=False, random_state=42)

# Apply the function to each row in the GeoDataFrame and create a new column 'distance_km'
df_right_join['distance_km'] = df_right_join.apply(calculate_distance, axis=1)

df_right_join['distance_km'].describe()

df_right_join

# only keep the relevant rows
df_right_join = df_right_join[['latitude_right', 'longitude_right', 'brightness', 'confidence', 'bright_t31', 'fwi', 'distance_km', 'date', 'geometry']]

df_right_join.to_file(os.path.join(folder, f'merged_eu_2013_right_downsampled_gdf'), driver='GPKG')

df_right_join = gpd.read_file(os.path.join(folder, 'merged_eu_2013_right_downsampled_gdf'))
df_right_join['longitude_right'].describe()

"""## 1.5 Assign negative labels (ignited) to the data points with distance > 31 * sqrt(2) / 2 and only keep rows with negative labels"""

# Drop rows with distance_km > 31 km
df_right_join = df_right_join[df_right_join['distance_km'] > 31 * math.sqrt(2) / 2 ]
# Drop rows with confidence < 30
df_right_join = df_right_join[df_right_join['confidence'] >= 30]

df_right_join.shape

df_right_join['ignited'] = False

"""## 1.6 Merge data and save the geo dataframe and save the dataframe"""

df_right_join

# Rename the columns to match
df_left_join = df_left_join.rename(columns={'latitude_left': 'latitude', 'longitude_left': 'longitude'})
df_right_join = df_right_join.rename(columns={'latitude_right': 'latitude', 'longitude_right': 'longitude'})

# Concatenate the GeoDataFrames
gdf_concat = pd.concat([df_left_join, df_right_join], ignore_index=True)

# Ensure the concatenated DataFrame is still a GeoDataFrame
gdf_concat = gpd.GeoDataFrame(gdf_concat, geometry='geometry')
gdf_concat



"""# 2. Add land cover (aggregated) data to pixel according to lon and lat

## 2.1 Load land cover shapefile and convert to geopandas dataframe
"""

land_cover_folder = '../../climada_petals/data/wildfire/land_cover/'
land_cover_raster = rasterio.open(os.path.join(land_cover_folder, 'land_cover2015.tif'))


# Define a function to get raster values at given points
def get_raster_values(raster, points):
    # Transform coordinates to raster's space
    coords = [(pt.x, pt.y) for pt in points.geometry]
    # Sample the raster at each coordinate
    return [x[0] for x in raster.sample(coords)]

# Apply the function to the geodataframe
gdf_concat['land_cover'] = get_raster_values(land_cover_raster, gdf_concat)

gdf_concat.land_cover.unique()

"""## 2.2 for each pixel (1x1 for left and 31x31 for right), calculate the average probability of fire ignition according to Clamada Master thesis Table 2.1

"""

# from shapely.geometry import Point, box
#
# # Create a new column 'geometry' that converts each point to a square polygon
# # Assuming the points are in a projected coordinate system where units are meters
# merged_eu_2013_gdf['geometry'] = merged_eu_2013_gdf.apply(
#     lambda row: box(
#         row['longitude'] - 500,  # Subtract 500 meters from the longitude
#         row['latitude'] - 500,   # Subtract 500 meters from the latitude
#         row['longitude'] + 500,  # Add 500 meters to the longitude
#         row['latitude'] + 500    # Add 500 meters to the latitude
#     ), axis=1
# )
# merged_eu_2013_gdf = gpd.GeoDataFrame(merged_eu_2013_gdf, geometry='geometry')
#
# import geopandas as gpd
#
# # Load the land cover shapefile
# land_cover_gdf = gpd.read_file('/path/to/ProbaV_UTM_LC100_biome_clusters_V3_global.shp')
#
# # Ensure CRS match, reproject if necessary
# if merged_eu_2013_gdf.crs != land_cover_gdf.crs:
#     merged_eu_2013_gdf = merged_eu_2013_gdf.to_crs(land_cover_gdf.crs)
#
# # Perform spatial join
# joined_gdf = gpd.sjoin(merged_eu_2013_gdf, land_cover_gdf, how='inner', op='intersects')
#
# import pandas as pd
#
# # Group by the index of the merged_eu_2013_gdf and find the most common land cover type
# most_common_land_cover = joined_gdf.groupby('index_merged_eu_2013').apply(
#     lambda g: g['bc_id'].value_counts().idxmax()
# )
# merged_eu_2013_gdf['most_frequent_land_cover'] = most_common_land_cover
#
# # Save the updated GeoDataFrame
# merged_eu_2013_gdf.to_file('/path/to/save/updated_merged_eu_2013.gpkg', driver='GPKG')

"""# 3. Add elevation data (2010, mean 7.5 arcsec) to pixel according to lon and lat
## 3.1 Load elevation shapefile and convert to geopandas dataframe

Create a Virtual Raster (VRT)
You can use the GDAL command-line tools or rasterio's virtual raster capabilities in Python to combine your GeoTIFF files into a VRT.
Using GDAL Command Line:
gdalbuildvrt combined_elevation.vrt 50N000E_20101117_gmted_mea075.tif 50N030W_20101117_gmted_mea075.tif 50N030E_20101117_gmted_mea075.tif 30N030W_20101117_gmted_mea075.tif 30N000E_20101117_gmted_mea075.tif 30N030E_20101117_gmted_mea075.tif
"""

# Method 2

# Your elevation folder path and file names
elevation_folder = '../../climada_petals/data/wildfire/elevation/'
files = [
    '50N000E_20101117_gmted_mea075.tif',
    '50N030W_20101117_gmted_mea075.tif',
    '50N030E_20101117_gmted_mea075.tif',
    '30N030W_20101117_gmted_mea075.tif',
    '30N000E_20101117_gmted_mea075.tif',
    '30N030E_20101117_gmted_mea075.tif'
]

# Open all files
src_files = [rasterio.open(os.path.join(elevation_folder, f)) for f in files]

# Create a virtual mosaic (in-memory, no VRT file written)
mosaic, out_transform = merge(src_files)





def find_nearest_valid_elevation(dataset, x, y):
    row, col = dataset.index(x, y)  # Convert geographic coordinates to raster indices
    max_distance = 10  # Define max distance to search for a valid elevation

    # Ensure initial indices are within bounds
    row = max(0, min(dataset.height - 1, row))
    col = max(0, min(dataset.width - 1, col))

    for dist in range(1, max_distance + 1):
        # Calculate window bounds, ensuring they are within the raster boundaries
        row_start = max(0, row - dist)
        row_stop = min(dataset.height, row + dist + 1)
        col_start = max(0, col - dist)
        col_stop = min(dataset.width, col + dist + 1)

        # Create the window using the bounds
        window = Window.from_slices((row_start, row_stop), (col_start, col_stop))
        data = dataset.read(1, window=window)
        valid_data = data[data != dataset.nodata]

        if valid_data.size > 0:
            return np.min(valid_data)  # Return the closest valid data
    return None  # Return None if no valid data found within the search radius


with MemoryFile() as memfile:
    with memfile.open(driver='GTiff', height=mosaic.shape[1], width=mosaic.shape[2],
                      count=1, dtype=mosaic.dtype, transform=out_transform,
                      crs=src_files[0].crs) as dataset:
        dataset.write(mosaic[0], 1)

        # Ensure GeoDataFrame is in the same CRS as the dataset
        gdf_concat = gdf_concat.to_crs(dataset.crs)

        # Sample the raster at each point location in the GeoDataFrame
        elevation_values = []
        for idx, row in gdf_concat.iterrows():
            x, y = row['geometry'].x, row['geometry'].y
            row, col = dataset.index(x, y)  # Get the raster indices

            if 0 <= row < dataset.height and 0 <= col < dataset.width:
                value = dataset.read(1, window=Window(col, row, 1, 1))[0, 0]
                if value == dataset.nodata:  # Check if the value is nodata
                    value = find_nearest_valid_elevation(dataset, x, y)
            else:
                value = find_nearest_valid_elevation(dataset, x, y)

            elevation_values.append(value)

        # Add the elevation data to the GeoDataFrame
        gdf_concat['elevation'] = elevation_values

        # Print bounds and metadata if needed
        print('Bounds:', dataset.bounds)
        print('Metadata:', dataset.meta)

# Now 'gdf' has an additional column 'elevation' with the elevation values

"""## 3.2 for each pixel (1x1 for left and 31x31 for right), calculate the average elevation"""

gdf_concat

gdf_concat['longitude'].describe()

# Save the updated GeoDataFrame
gdf_concat.to_file(os.path.join(folder, 'ignited_eu_2013_gdf'), driver='GPKG')