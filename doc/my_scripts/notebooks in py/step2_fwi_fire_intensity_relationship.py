# -*- coding: utf-8 -*-
"""step2_fwi_fire_intensity_relationship.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JcxrQ9S_VloZBXv9nTrLF59b9aROogxi
"""

import xarray as xr
import numpy as np
import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt
import contextily as ctx
import os
import math

"""# 1. Find correlation between FWI and fire intensity"""

save_path = '../../climada_petals/data/wildfire/output/2013/'
merged_gdf = gpd.read_file(os.path.join(save_path, 'merged_uk_2013_left_gdf'))
# The distance does not consider the curvature of the Earth and is in degree not km
# Ex. distance of the first row is calculated by
# np.sqrt((merged_gdf['latitude_left'][0] - merged_gdf['latitude_right'][0])**2 + (merged_gdf['longitude_left'][0] - merged_gdf['longitude_right'][0])**2)
merged_gdf

print(merged_gdf.shape)

from geopy.distance import great_circle

# Define a function to calculate the distance
def calculate_distance(row):
    coords_1 = (row['latitude_left'], row['longitude_left'])
    coords_2 = (row['latitude_right'], row['longitude_right'])
    return great_circle(coords_1, coords_2).kilometers

# Apply the function to each row in the GeoDataFrame and create a new column 'distance_km'
merged_gdf['distance_km'] = merged_gdf.apply(calculate_distance, axis=1)
merged_gdf

merged_gdf['distance_km'].describe()

merged_gdf['confidence'].describe()

"""## 1.1 Data Cleaning"""

# Drop FWI and brightness rows with missing values
filtered_df = merged_gdf.dropna(subset=['fwi', 'brightness'])
# Drop rows with distance_km > 31 km
filtered_df = filtered_df[filtered_df['distance_km'] <= 31 * math.sqrt(2) / 2]
# Drop rows with confidence < 30
filtered_df = filtered_df[filtered_df['confidence'] >= 30]

filtered_df

"""## 1.2 Calculate Pearson correlation coefficient without normalization"""

# calculate pearson correlation coefficient
correlation = filtered_df['fwi'].corr(filtered_df['bright_t31'])
print(f"The correlation between FWI and brightness is: {correlation}")

correlation = filtered_df['fwi'].corr(filtered_df['brightness'])
print(f"The correlation between FWI and brightness is: {correlation}")

def histogram_intersection(a, b):
    v = np.minimum(a, b).sum().round(decimals=1)
    return v
filtered_df[['fwi', 'bright_t31']].corr(method=histogram_intersection)

import seaborn as sns
# Create a scatter plot using seaborn with 'fwi' on the x-axis, 'brightness' on the y-axis, and 'distance_km' as the hue
plt.figure(figsize=(10, 8), dpi=500)
scatter = sns.scatterplot(
    data=filtered_df,
    x='fwi',
    y='brightness',
    hue='distance_km',
    palette='viridis',
    alpha=0.7,
    size='distance_km',
    sizes=(1, 15)
)
plt.xlabel('FWI')
plt.ylabel('Brightness')
plt.title('Scatter Plot of FWI vs Brightness with Distance as Colormap')
plt.show()

import seaborn as sns
# Create a scatter plot using seaborn with 'fwi' on the x-axis, 'brightness' on the y-axis, and 'distance_km' as the hue
plt.figure(figsize=(10, 8), dpi=500)
scatter = sns.scatterplot(
    data=filtered_df,
    x='fwi',
    y='bright_t31',
    hue='distance_km',
    palette='viridis',
    alpha=0.7,
    size='distance_km',
    sizes=(1, 15)
)
plt.xlabel('FWI')
plt.ylabel('bright_t31')
plt.title('Scatter Plot of FWI vs Brightness with Distance as Colormap')
plt.show()

# Define a scaling factor to make the markers smaller
scaling_factor = 15

plt.figure(figsize=(10, 8), dpi=500)
plt.scatter(filtered_df['fwi'], filtered_df['bright_t31'], s=filtered_df['distance_km'] / scaling_factor, alpha=0.5)
plt.xlabel('FWI')
plt.ylabel('bright_t31')
plt.title('Scatter Plot of FWI vs Brightness with Distance as Marker Size')

"""Pearson correlation > 0.25 is considered a very strong association (Akoglu, 2018). The correlation should be higher if geolocation is considered (calculate correlation separately for different areas).

## 1.3 Calculate Pearson correlation coefficient with normalization
"""

# Normalize the data using Min-Max Scaling
# x_norm = (x - x_min) / (x_max - x_min)
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
filtered_df[['fwi', 'brightness']] = scaler.fit_transform(filtered_df[['fwi', 'brightness']])

# Calculate the correlation matrix
correlation_matrix = filtered_df[['fwi', 'brightness']].corr()

# Get the correlation between 'fwi' and 'brightness'
correlation_fwi_brightness = correlation_matrix.loc['fwi', 'brightness']

# Display the correlation
print(f"Correlation between 'fwi' and 'brightness': {correlation_fwi_brightness}")
print(correlation_matrix)









"""# 2. Find relationship between FWI and fire intensity

## 2.1 Link FWI and brightness using multiple Functions

In Step 1, we will explore multiple functional relationships between fwi and brightness to determine the best representation. We can try linear, polynomial, logarithmic, and exponential functions. Then, we will evaluate the goodness of fit for each function using metrics like R-squared.

1. Linear Function: y = a * x + b
2. Polynomial Function (degree 2): y = a * x^2 + b * x + c
3. Polynomial Function (degree 3): y = a * x^3 + b * x^2 + c * x + d
4. Logarithmic Function: y = a * log(x) + b
5. Exponential Function: y = a * exp(b * x)
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
from scipy.optimize import curve_fit

# Define functions
def linear(x, a, b):
    return a * x + b

def polynomial2(x, a, b, c):
    return a * x**2 + b * x + c

def polynomial3(x, a, b, c, d):
    return a * x**3 + b * x**2 + c * x + d

def logarithmic(x, a, b):
    return a * np.log(x) + b

def exponential(x, a, b):
    return a * np.exp(b * x)

# Prepare data
x = filtered_df['fwi']
y = filtered_df['brightness']

# Fit functions and calculate R-squared
def fit_and_evaluate(func, x, y):
    popt, pcov = curve_fit(func, x, y, maxfev=10000)
    y_pred = func(x, *popt)
    r2 = r2_score(y, y_pred)
    return popt, r2

# Linear
popt_linear, r2_linear = fit_and_evaluate(linear, x, y)

# Polynomial degree 2
popt_poly2, r2_poly2 = fit_and_evaluate(polynomial2, x, y)

# Polynomial degree 3
popt_poly3, r2_poly3 = fit_and_evaluate(polynomial3, x, y)

# Logarithmic
popt_log, r2_log = fit_and_evaluate(logarithmic, x, y)

# Exponential
popt_exp, r2_exp = fit_and_evaluate(exponential, x, y)

# Collect results
results = {
    'Linear': r2_linear,
    'Polynomial (degree 2)': r2_poly2,
    'Polynomial (degree 3)': r2_poly3,
    'Logarithmic': r2_log,
    'Exponential': r2_exp
}

best_function = max(results, key=results.get)
best_r2 = results[best_function]

# Print results
print(f"Best function: {best_function} with R-squared: {best_r2}")
print("All R-squared values:")

for func, r2 in results.items():
    print(f"{func}: {r2}")

# Plotting
plt.figure(figsize=(10, 6))
plt.scatter(x, y, s=10, label='Data')

# Plot best fit line
if best_function == 'Linear':
    plt.plot(x, linear(x, *popt_linear), color='red', label='Linear fit')
elif best_function == 'Polynomial (degree 2)':
    plt.plot(x, polynomial2(x, *popt_poly2), color='red', label='Polynomial (degree 2) fit')
elif best_function == 'Polynomial (degree 3)':
    plt.plot(x, polynomial3(x, *popt_poly3), color='red', label='Polynomial (degree 3) fit')
elif best_function == 'Logarithmic':
    plt.plot(x, logarithmic(x, *popt_log), color='red', label='Logarithmic fit')
elif best_function == 'Exponential':
    plt.plot(x, exponential(x, *popt_exp), color='red', label='Exponential fit')

plt.xlabel('FWI')
plt.ylabel('Brightness')
plt.title(f'Scatter Plot of FWI vs Brightness with {best_function} fit')
plt.legend()
plt.show()





"""## 2.2 Use Statistical Methods"""

# Calculate the correlation coefficient
correlation = filtered_df['fwi'].corr(filtered_df['brightness'])
print(f"Correlation coefficient between 'fwi' and 'brightness': {correlation}")

import statsmodels.api as sm
# Perform a regression analysis using statsmodels
X = sm.add_constant(filtered_df['fwi'])  # Add a constant term for the intercept
y = filtered_df['brightness']

model = sm.OLS(y, X).fit()
predictions = model.predict(X)

# Print out the regression analysis summary
print(model.summary())

# Print the R-squared value explicitly
print(f"R-squared: {model.rsquared}")

"""R-squared: 0.063 - This indicates that approximately 6.3% of the variability in brightness can be explained by fwi.

Prob (F-statistic): 1.68e-197 - This is the p-value for the F-statistic. A value close to zero (much less than 0.05) indicates that the model is statistically significant.


The regression model shows that there is a statistically significant relationship between fwi and brightness, with both the intercept and the slope being highly significant (p < 0.05). However, the R-squared value of 0.063 indicates that fwi explains only about 6.3% of the variability in brightness, suggesting that other factors may also influence brightness.

## 2.3 Use Machine Learning Methods
"""

# from sklearn.model_selection import train_test_split
# from sklearn.linear_model import LinearRegression
# from sklearn.metrics import mean_squared_error, r2_score
#
# # Split the data into training and testing sets
# X = filtered_df[['fwi']]
# y = filtered_df['brightness']
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
#
# # Initialize and train the model
# model = LinearRegression()
# model.fit(X_train, y_train)
#
# # Make predictions
# y_pred = model.predict(X_test)
#
# # Evaluate the model
# mse = mean_squared_error(y_test, y_pred)
# r2 = r2_score(y_test, y_pred)
#
# print(f"Mean Squared Error: {mse}")
# print(f"R^2 Score: {r2}")





"""# 3. Include WFI into overall (global) propagation probability (self.ProbaParams.prop_proba=0.21)

A simple thought:
Step1: Calculate the average FWI
Step2: For each centriod, calculate the percentage deviation of FWI from the average FWI
Step3: Use the percentage deviation * 0.21 to adjust the overall propagation probability
Step4: Consider geographical location using kNN or other methods...
"""

